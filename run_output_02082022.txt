apb121@vertex53:latency_tool$ ./latency_tool class_parser.hpp user_options.hpp binary_analyser.hpp cache_info.hpp latency_tool

===========================================================

CACHE ANALYSIS

===========================================================


=== L1 data cache ===

Size: 32768 bytes
Associativity: 8
Linesize: 64 bytes
Critical stride: 4096 bytes

=== L1 instruction cache ===

Size: 32768 bytes
Associativity: 8
Linesize: 64 bytes
Critical stride: 4096 bytes

=== L2 cache ===

Size: 262144 bytes
Associativity: 4
Linesize: 64 bytes
Critical stride: 65536 bytes

=== L3 cache ===

Size: 12582912 bytes
Associativity: 16
Linesize: 64 bytes
Critical stride: 786432 bytes

=== L4 cache ===

This processor does not have an L4 cache


===========================================================

SOURCE CODE ANALYSIS

===========================================================

This tool analyses your source code files to discover cache-inefficient data layouts in user-defined types.
Any user-defined types which can be optimised by reordering their data members will be reported, along with suggestions for new layouts.

Additionally, this tool will identify any user-defined types where the Least Common Multiple of the size of the type and the critical stride of your processor is small.
If objects of these types are stored in contiguous memory (such as an array or vector), the same data members in different objects may compete for space in the cache.
If access patterns involve accessing the same data members across multiple contiguous objects, this may create latency problems.

Analysing your source code...

No data ordering or type-size inefficiencies have been detected!

===========================================================

BINARY ANALYSIS

===========================================================

Groups of functions that call each other and compete for the same portion of the cache may cause latency problems.
If the number of functions in the group is above the associativity of the instruction cache (8 on this processor), these functions will definitely evict each other from the cache.
If the number of functions in the group is equal to or slighly below the associativity of the instruction cache, there is a significant probability that the functions will evict each other from the cache. The uncertainty (standard deviation) of this probability can also be a source of jitter at runtime.
The larger the region of cache memory that the functions compete for, the more code needs to be fetched from lower level caches the next time the evicted function needs to be executed, and the more significant the latency penalties are.

Analysing your binary...


=== Large function detected ===

Function name: UserOptions::parse_flags(int, char**)

This function spans between 1 and 2 critical strides. This means that, when it executes, it will occupy 2 cache ways, and may therefore compete with itself for cache space.


=== Large function detected ===

Function name: UserOptions::run_cache_setup()

This function spans between 1 and 2 critical strides. This means that, when it executes, it will occupy 2 cache ways, and may therefore compete with itself for cache space.


=== Large function detected ===

Function name: FileCollection::detect_types(std::bitset<8ul>&)

This function spans between 1 and 2 critical strides. This means that, when it executes, it will occupy 2 cache ways, and may therefore compete with itself for cache space.


=== Large function detected ===

Function name: Binary::find_problem_function_groups(UserOptions&)

This function spans between 1 and 2 critical strides. This means that, when it executes, it will occupy 2 cache ways, and may therefore compete with itself for cache space.


===== Function group analysis =====

The following groups of functions have been identified as potential sources of latency problems.
They have been ranked based on a combination of the number of functions in the group and the amount of cache space they compete for.
This has been calculated based on the instruction-cache's critical stride of 4096 bytes and associativity of 8, as well as a coexecution indirection level of 1 and competition overlap threshold of 256 bytes.

=== Group ===

FileCollection::get_alignment(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)
FileCollection::detect_types(std::bitset<8ul>&)

These 2 coexecuting functions call each other (directly) 2 times and compete for the same 1984-byte region of the cache

=== Group ===

Binary::find_problem_function_groups(UserOptions&)
Binary::rec_problem_find(unsigned long, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> >&, int)

These 2 coexecuting functions call each other (directly) 2 times and compete for the same 649-byte region of the cache



===== Description =====

The largest identified group of coexecuting functions that compete for cache space contains 2 functions.

The size of this group of functions is well below the associativity of the L1 instruction cache, and should therefore not be a source of serious latency issues. It may still be worth trying to reduce the competition between these functions by following the suggestions below.


===== Suggestions =====

There are a number of things that programmers can do to mitigate problems that might be identified in this analysis.

(1) rewrite the functions to involve less code!
(2) inline small functions that may evict parts of larger functions from the cache so that they do not conflict for cache space
(3) combine smaller functions that compete for cache space into fewer, longer functions
(4) move functions around in the code so that functions that are called together are placed together in memory and will therefore occupy non-overlapping regions of the cache
(5) try the gcc flag --falign-functions=<alignment>, which will force functions onto the specified alignment and which may improve the situation - but may also make it worse!