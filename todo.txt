-- make a readme!

-- report section
    -- intro
        -- general description of tool
            -- analyses cache (I will discuss this in section ...)
            -- reads source code and binary file with debugging symbols
            -- analyses data cache efficiency
                -- finds inefficiently ordered types
                -- finds inefficiently sized types (I will discuss this in section ...)
            -- analyses instruction cache inefficiency
                -- finds inefficiently sized functions
                -- finds groups of functions that may compete for cache space
        -- as far as I can find, no tool that does anything like this exists currently
            -- there are tools that you can use to look at functions in binary files and the structure of classes (nm, objdump, gdb), but nothing that brings this information together and uses it to (a) identify potential sources of data/instruction cache inefficiency and (b) suggest solutions to these problems
    -- programme setup
        -- checks for valid command line arguments
        -- checks that the specified files exist
        -- checks for programme requirements
            -- these are tailored to the specific user options
                -- e.g., g++ and perf are only required if empirical tests are chosen, gdb is only required if source code files are provided
                -- nm and objdump are always required
        -- parsing flags
    -- cache analysis
        -- code class structure
            -- currently only performs L1 cache analysis but extensibile to lower cache levels
        -- default: checks for system variables (sysconf)
            -- (show code)
            -- sometimes dimensions are just... missing from the system variables
                -- show example, e.g., from home desktop
            -- if missing, default behaviour is to perform empirical tests based on part 1 of the project
                -- basically does the tests from the data section
                    -- compiles some test files
                        -- uses perf on them
                            -- using perf and analysing cache misses / cache miss %
                    -- data cache associativity
                        -- show code
                        -- setup
                            -- g++ -g ./test_files/associativity_test_d.cpp -o ./temp_files/associativity_test_d
                            -- perf stat -x , --append -o ./temp_files/assoctmpd.txt -e L1-dcache-load-misses -r 1000 ./temp_files/associativity_test_d <16 or 65536 alignment>
                        -- best measure: cache miss percentage
                            -- minimum of
                                -- first jump in 65536 (raw cache miss percentage)
                                -- highest standard deviation in cache miss percentage
                                    -- show graphs from first part
                    -- data cache critical stride (even though you can work out from associativity)
                        -- show code
                        -- setup
                            -- g++ -g ./test_files/critical_stride_test_d.cpp -o ./temp_files/critical_stride_test_d
                            -- perf stat -x , --append -o ./temp_files/cstmpd.txt -e L1-dcache-loads,L1-dcache-load-misses,duration_time -r 1000 ./temp_files/critical_stride_test_d <64, 128, 256, ... 65536>
                        --best measure:
                            -- highest standard deviation of cache miss percentage is a quarter of the critical stride
                    -- instruction cache associativity
                        -- show code
                            -- g++ -g -falign-functions=65536 ./test_files/associativity_test_i.cpp -o ./temp_files/associativity_test_i_65536 
                            -- g++ -g ./test_files/associativity_test_i.cpp -o ./temp_files/associativity_test_i_noalign 
                            -- perf stat -x , --append -o ./temp_files/assoctmpi.txt -e L1-icache-load-misses -r 1000 ./temp_files/associativity_test_i_65536 
                            -- perf stat -x , --append -o ./temp_files/assoctmpi.txt -e L1-icache-load-misses -r 1000 ./temp_files/associativity_test_i_noalign 
                        -- best measure:
                            -- same as data cache associativity
                    -- instruction cache critical stride
                        -- show code
                        -- setup
                            -- g++ -g -falign-functions= <64, 128, 256, ... 65536> ./test_files/critical_stride_test_i.cpp -o ./temp_files/critical_stride_test_i_
                            -- perf stat -x , --append -o ./temp_files/cstmpi.txt -e L1-icache-load-misses -r 1000 ./temp_files/critical_stride_test_i_
                        -- best measure:
                            -- last large jump in absolute number of instruction cache misses (can't view standard deviation...)
                    -- empirical tests general
                        -- how to choose critical stride direct or size / assoc
                            -- powers of two? (see code)
                -- have tested this on many lab machines, my own laptop, my own decade-old desktop
        -- user configuration
            -- user can choose to do have the programme just print cache dimensions
            -- user can choose a specific cache structure
                -- user can also choose 'default' cache (4096, 8, 64)
            -- user can choose not to perform empirical cache tests
                -- if necessary, defaults will be used (user is warned when this happens)
            -- user can choose to use existing empirical temp files (cf. -k flag, which is also automatically set with -e)
                -- user is informed and programme terminates if the user wants to perform some analysis which requires temp files that don't exist
    -- data analysis
        -- original attempt (got quite far but major problems)
            -- overall
                -- detecting classes
                    -- regex: "(class|struct)\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*(:\\s*(public|protected|private)\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*)?\\{"
                        -- recursion to find balanced brackets
                            -- regular expressions, like FSMs cannot do counting
                                -- so it's only really possible to detect the start of a class with regex
                                    -- end of a class: } <variable name> ;
                -- detecting types
                    -- regex "(?:(?:virtual *|auto *|static *|const *|unsigned *|signed *|register *|volatile *|void *\\* *|bitset *<.*> *|array *<.*> *|std::vector *<.*> *|deque *<.*> *|forward_list *<.*> *|list *<.*> *|stack *<.*> *|queue *<.*> *|priority_queue *<.*> *|set *<.*> *|multiset *<.*> *|map *<.*> *|multimap *<.*> *|unordered_set *<.*> *|unordered_multiset *<.*> *|unordered_map *<.*> *|unordered_multimap *<.*> *|size_t *|std::string *|short *|long *|char *|wchar_t *|char8_t *|char16_t *|int *|float *|double *| bool *|complex *)+)[\\*]*(?: +\\*?\\*? *)( *const *)?([a-zA-Z_][a-zA-Z0-9_]*) *(([{;,=])|(((\\[ *[0-9]* *\\])+)))"
                        -- putting user defined types in the middle!
                -- calculate their sizes
                -- calculating their alignments
                -- working out the size of the whole class (including vtable pointers!)
            -- challenges
                -- class hierarchies
                    -- represent these with pointers to other objects of UDType type
                        -- use these pointers when calcualating size
                -- user defined types containing other types
                    -- iteratively fill out these values until none are left un-calculated
                -- code parsing issues
                    -- ignoring things in speech marks... but not based on speech marks that are themselves in speech marks!
                    -- function parameters
                        -- func(int a = 5, char g = 'f')
                    -- variable declarations inside functions (especially inline)
                        -- solution: ignore anything in curly brackets
                    -- pure virtual functions
                        -- virtual int function = 0;
                            -- solution: keep virtual in the regex and discard later
                -- size calculation issues
                    -- (many)
                    -- vector<normal> vs vector<bool>
                    -- pointers
                        -- asterisks inside template parameters vs outside them
                            -- vector<string*> vs vector<string>*
                        -- char const * const * c;
                    -- array lengths
                        -- char a[5]
                        -- vector<char[5]> vs vector<char>[5]
                        -- array<array<char[5], 6>, 7>[3]
                            -- also alignment! aligns by char
                    -- static variables
                -- issues that never got solved
                    -- comma separated declarations
                    -- add template functionality
                        -- suggest specialisations?
                    -- many-level recursive array<>s
                    -- slash-star comments... (see both/all places where characters (usu. brackets) in comments are ignored)
                    -- slash-slash comments
        -- second attempt
            -- using llvm-dwarfdump
                -- show file
            -- problem
                -- still complex parsing, just of a different kind
                -- files MUCH too large to process in a timely fashion
        -- third time lucky:
            -- glean class names from source code files with regex from before
                -- "(class|struct)\\s*([a-zA-Z_][a-zA-Z0-9_<>]*)\\s*(:\\s*(public|protected|private)\\s*([a-zA-Z_][a-zA-Z0-9_<>]*)\\s*)?\\{"
                -- create list of class names
            -- create a file of gdb commands to batch
                -- info types ClassName
                -- info types ClassName<.*>
                -- info types ClassName2
                -- info types ClassName2<.*>
                -- ...
            -- then run:
                -- gdb -x ./temp_files/gdbinfocmdtmp.txt -batch binary_file_path > ./temp_files/gdbinfoouttmp.txt
            -- the result lists all the class names including templated types
                -- solves the templating problem from earlier
            -- parse this file into a vector of files and create another file:
                -- ptype /o TypeName1
                -- ptype /o TypeName2<type3>
                -- ptype /o TypeName2<type4>
                -- ...
            -- then run:
                -- gdb -x ./temp_files/gdbtypecmdtmp.txt -batch binary_file_path > ./temp_files/gdbtypeouttmp.txt
            -- creates a file with names of classes, names of their members with sizes and offsets!
                -- parse this
                -- still need to calculate alignments because gdb does not provide these
                    -- show code
            -- still also have to code a size calculator (which is somewhat challenging with alignments, etc.) to calculate the size of proposed new orderings
                -- then order by size (greedy algorithm is correct, I believe)
            -- if the new ordering is smaller than the current one, an inefficient ordering is detected
                -- inform the user!
            -- additional functionality:
                -- looks at the total size of a class (including things like vtable pointers. which don't show up in source code or in gdb
                    -- lcm calculation, informs user of how many elements contiguously in an array would cause a problem
    -- binary analyser
        -- identify functions from the binary
            -- nm -v -C -l --radix=d --print-size file_path > ./temp_files/nmtmp.txt
                -v ordering by address
                -C name demangler
                -- (note duplicates... and use of map/set)
            -- show nm output
            -- some considerations
                -- consider: overloading
                    -- solved by using gdb, including parameters in function 'names'
                -- consider: templates!
            -- identify all user functions using:
                -- type T
                -- type W with UserType:: on the front
                    -- this takes input from the class analyser
                        -- (consider: flow of information)
            -- function representation:
                -- address
                -- size
                -- name
                -- file location
            -- from this information you can make suggestions to users based on their processor configuration
                -- compare size of functions to cache critical stride
                    -- identify potential functions that compete with themselves
        -- identify relations between functions
            -- objdump -d -C -Mintel --no-show-raw-insn binary_file_path > ./temp_files/objdumptmp.txt
                -d disassemble
                -C demangle
            -- show objdump output
                -- very basic assembly parsing to find function calls
                    -- (development possibility: identify where there are, e.g., loops that make some coexecutions stronger than others)
        -- use this objdump information to identify which functions call which other functions
            -- use this as an analysis of 'coexecution'
            -- better to use objdump for many reasons
                -- a lot of function calls (e.g. to constructors, assignment operators, multiple functions from templates, etc.) are not explicit in source code
                    -- particularly useful because it can identify slow-downs that could never been seen just by looking at the source code. constructors might conflict!
                -- avoids wasting time on functions written but never called (these are not compiled into the binary)
        -- use the nm information to identify which functions compete for cache space
            -- (use address and size)
        -- for each function, put together the competes-with and coexecutes-with data
            -- then: use depth-first search on these to look for groups of functions where all the functions coexecute and compete for cache space
                -- identify how much cache space they compete for
                -- identify how often they call each other
                -- identify the size of the group of functions
                    -- put all this together to come up with a group score
                        -- (development possibility: more empirically backed up way to score functions)
        -- print out these groups (in order of score) with the information about them (so the user can also make judgements about which groups to target)
            -- suggestions
                -- inline some functions (then they will not conflict! may also have other advantages...)
                -- move functions around (put function that are called together together)
                -- combine functions
                -- try -falign-functions=...
                -- (forcibly stop some functions coming into the cache with non temporal code accesses??)
        -- user configuration
            -- competition thresholds
                -- how much cache space a group must compete for
                -- (also how these are used to improve efficiency)
            -- coexecution thresholds
                -- how many levels of coexecution indirection
                -- For example, given functions A(), B(), C(), and D() where A calls B, B calls C, and C calls D, A and B would always be considered to coexecute, A and C would be considered to coexecute only with at least 1 level of indirection, and A and D would be considered to coexecute only with at least 2 levels of indirection.
            -- ranking length flag
            -- all functions flag
                -- library functions
    -- other
        -h
        -- Warnings about options that may cause long execution times
            -- e.g.,
                -- high indirection numbers
                -- low competition thresholds
                -- large ranking lengths
        -- Warnings when important values are missing
        -- Warnings when default values are chosen
            -- (falls back on defaults instead of terminating as much as possible)
        -- lots of robustness checks
    -- some interesting analysis the programme provides
        -- the number of groups of particular sizes at, e.g., 256, 512, 1024, 2048, 4096 aligned functions is a new way to visualise the cache problems associated with those alignments
            -- specifically
                -- at 256, there is one competing coexecuting group of 3 functions, and many of 2 and below
                -- at 512, there is one competing coexecuting group of 5 functions, and many of 4 and below
                -- at 1024, there is one group of 9, and several of 8, etc.
        -- also demonstrate that simply moving functions around in the code can change the number of problem functions
            -- compare old analyses to new ones
        -- interesting demonstrations of function group identifier
            -- compare aligned functions perfomance with nCr (e.g., groups of size 3 from 17 competing and coexecuting functions (1 extra level of indirection) leads to 680 groups found!)
            -- compare critical stride tests 1024, 2048, and 4096!
                -- note also that for 1024, you get groups at 4-function intervals!
            -- cool: for the critical stride 1024 test and looking for groups of 9 ... there is 1! (for groups of 8 there are 12 groups)
            -- when commenting out the function "for (auto& i : problem_groups)", the number of 6-large groups of competing functions goes down from 14 to 6
            -- interesting to compare old run outputs of competing function groups and newer ones

    -- all feeds back to suggestions that are tailored to
        -- specific source code
        -- specific compiled binary
        -- specific processor
            -- but all these options are customisable and toggleable
                -- possible to make the suggestions generic/default
        -- flow of information
            -- esp: how different parts use each others' information
                -- cache analysis
                    -- source code analysis uses cache analysis
                    -- binary analysis uses cache analysis and the output of the source code analysis (class names)

-- other development ideas
    -- make the temp files only overwrite the old temp file at the end (with cp)
        -- so that if the programme is aborted halfway it isn't a problem...
    -- might be better if the binary is the first file?
        -- maybe not, because this way it is easy to do binary-only without having a flag for it
    -- give the user the option to combine the system variables and known, manually-input specific values ??
    -- identify and warn the user about discontiguous data structures
    -- empirical cache tests
        -- more sophisticated stddev checking for spikes?